\documentclass[12pt]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{bm} %bold symbols
\usepackage{hyperref} %add links

%%%%%%%%%%%%%%%%%%%%%%%%%
% 	Define vars here 	%
%%%%%%%%%%%%%%%%%%%%%%%%%

\def\hwName{Homework Set Part B}
\author{Zhengyu James Pan} %use like \@author
\def\instructor{Dr. Paul Kessenich}
\def\email{jzpan@umich.edu}
\def\dueDate{???}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\realsn}{\reals^{n}}
\newcommand{\nxn}{n\times n}
\newcommand{\realsnxn}{\reals^{\nxn}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\transpose}{^\top}

\makeatletter

\begin{document}
%Header
\pagestyle{head}
\firstpageheader{}{}{}
\header{MATH 217}{\hwName}{\thepage}

%Solution formatting
\printanswers
\unframedsolutions

%Top matter
{\parindent0in
\bf
\begin{center}
	MATH 217 W24 - LINEAR ALGEBRA, Section 001 ({\instructor}) \\
	{\hwName} due {\dueDate} at 11:59pm \\
	\@author\ (\href{mailto:\email}{\email})
\end{center}
}

\begin{questions}
\question Question
	\begin{parts}
		\part Prove that $F$ is alternating if and only if $F (\vec u, \vec v) = -F (\vec v, \vec u)$ for all $\vec u, \vec v \in \reals^2$.
		\begin{solution}
			By bilinearity, we know 
            \begin{align*}
                F(u + v, v + u) &= 0 \\
                F(u, v + u) + F(v, v + u) &= 0 \\
                F(u, v) + F(u, u) + F(v, v) + F(v, u) &= 0 \\
                F(u, v) + 0 + 0 + F(v, u) &= 0 \\
                F(u, v) + F(v, u) &= 0 \\
                F(u, v) &= -F(v, u)
            \end{align*}
		\end{solution}
		\part Prove that if $F$ is alternating and $F (\vec e_1, \vec e_2) = 1$, then $F (\vec u, \vec v) = $ det$[\vec u \  \vec v]$ for all $\vec u, \vec v \in \reals^2$.
			\begin{solution}
				Express $\vec u$ and $\vec v$ as linear combinations of $e_1, e_2:$
				\[ \vec u = u_1 \vec e_1 + u_2 \vec e_2 \text{   and   } \vec v = v_1 \vec e_1 + v_2 \vec e_2 \]
				Then \begin{align*}
					F(\vec u, \vec v) &= F(u_1 \vec e_1 + u_2 \vec e_2, v_1 \vec e_1 + v_2 \vec e_2) \\
                    &= F(u_1 \vec e_1, v_1 \vec e_1 + v_2 \vec e_2) + F(u_2 \vec e_2, v_1 \vec e_1 + v_2 \vec e_2) \tag{bilinearity} \\
                    &= F(u_1 \vec e_1, v_1 \vec e_1) + F(u_1 \vec e_1, v_2 \vec e_2) + F(u_2 \vec e_2, v_1 \vec e_1 ) + F(u_2 \vec e_2, v_2 \vec e_2)\\
                    &= u_1 v_1 F( \vec e_1, \vec e_1) + u_1 v_2 F(\vec e_1, \vec e_2) + u_2 v_1 F(\vec e_2, \vec e_1 ) + u_2 v_2 F(\vec e_2, \vec e_2)\\
                    &= u_1 v_1 (0) + u_1 v_2 (1) + u_2 v_1 (-1) + u_2 v_2 (0) \tag{alternating} \\
                    &= u_1 v_2 - u_2 v_1\\
                    &= \text{det}\begin{bmatrix*}
                        u_1 & v_1 \\ u_2 & v_2
                    \end{bmatrix*} \\
                    &= \text{det}[\vec u\  \vec v]
				\end{align*}
			\end{solution}
	\end{parts}

\clearpage


\question \begin{parts}
    \part Prove that $T$ is a linear transformation.
        \begin{solution}
            Let $A, B \in \reals^{2 \times 2}$, and $c \in \reals$. \\
            $T$ respects addition:
            \[ T(A+B) = (A+B)M = AM + BM = T(A) + T(B) \]
            by distributivity of matrix multiplication.\\
            $T$ respects scalar multiplication:
            \[ T(cA) = (cA)M = c(AM) = cT(A) \]
            by properties of matrix multiplication.
            \par Since $T$ respects addition and scalar multiplication, it is linear.
        \end{solution}
    \part Find the $\mathcal E$-matrix $[T ]_\mathcal E$ of $T$, where $\mathcal E$ is the ordered basis
    \[ \mathcal E = \left(E_{11}, E_{12}, E_{21}, E_{22}\right) = \left( \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \right) \]
    of $\reals^{2 \times 2}$. Your answer should be in terms of the entries of $M$.
        \begin{solution}
            \begin{align*}
                [T]_\mathcal E &= \begin{bmatrix} | & | & | & | \\ [T(E_{11})]_\mathcal E & [T(E_{12})]_\mathcal E & [T(E_{21})]_\mathcal E & [T(E_{22})]_\mathcal E \\ | & | & | & | \end{bmatrix}\\\\
                [T(E_{11})]_\mathcal E &= \left[\begin{bmatrix*} a & b \\ 0 & 0 \end{bmatrix*}\right]_\mathcal E = \begin{bmatrix} a \\ b \\ 0 \\ 0 \end{bmatrix} \\
                [T(E_{12})]_\mathcal E &= \left[\begin{bmatrix*} c & d \\ 0 & 0 \end{bmatrix*}\right]_\mathcal E = \begin{bmatrix} c \\ d \\ 0 \\ 0 \end{bmatrix} \\
                [T(E_{21})]_\mathcal E &= \left[\begin{bmatrix*} 0 & 0 \\ a & b \end{bmatrix*}\right]_\mathcal E = \begin{bmatrix} 0 \\ 0 \\ a \\ b \end{bmatrix} \\
                [T(E_{22})]_\mathcal E &= \left[\begin{bmatrix*} 0 & 0 \\ c & d \end{bmatrix*}\right]_\mathcal E = \begin{bmatrix} 0 \\ 0 \\ c \\ d \end{bmatrix} \\\\
                [T]_\mathcal E = \begin{bmatrix} a & c & 0 & 0 \\ b & d & 0 & 0 \\ 0 & 0 & a & c \\ 0 & 0 & b & d \end{bmatrix}
            \end{align*}
        \end{solution}
    \part Compute det$[T ]_\mathcal E$.
		\begin{solution}
			Using the Laplace expansion on our result from (b), \
				\begin{align*}
					\text{det}[T]_\mathcal E &= \begin{vmatrix} a & c & 0 & 0 \\ b & d & 0 & 0 \\ 0 & 0 & a & c \\ 0 & 0 & b & d \end{vmatrix} \\
					&= a\begin{vmatrix} d & 0 & 0 \\  0 & a & c \\ 0 & b & d \end{vmatrix}  - c\begin{vmatrix} b & 0 & 0 \\ 0 &  a & c \\ 0 & b & d \end{vmatrix}\\\
					&= ad\begin{vmatrix}  a & c \\ b & d \end{vmatrix} - bc\begin{vmatrix}  a & c \\ b & d \end{vmatrix}\\
					&= (ad - bc)^2 \\
					&= a^2 d^2 - 2abcd + b^2 c^2 \\
				\end{align*}
		\end{solution}
	\part Compute det$[T ]_\mathcal B$.
		\begin{solution}
			The determinant of a transformation is the same in any basis. So it will be the same as part (c), or $(ad - bc)^2 = a^2 d^2 - 2abcd + b^2 c^2$.
		\end{solution}
	\part Either prove that T is always diagonalizable no matter what $M$ is, or provide an explicit example of a matrix $M$ for which $T$ is not diagonalizable and brieﬂy explain why your example works.
        \begin{solution}
            %FINISHHHHHHADSHFOAHFOHEIFHBIKFJBODJ
        \end{solution}
\end{parts}

\question \begin{parts}
	\part Prove that there exists a unique vector $\vec z \in \reals^4$ such that $T (\vec x) = \vec z \cdot \vec x$ for all $\vec x \in \reals^4$, and find the components of $\vec z$ in terms of the vectors $\vec u, \vec v$, and $\vec w$. (Hint: $\vec x = x_1 \vec e_1 + x_2 \vec e_2 + x_3 \vec e_3 + x_4 \vec e_4$.)
        \begin{solution}
            We calculate the determinant by by expanding the column of $\vec x$. 
            \begin{align*}
                \begin{vmatrix} \vec x & \vec u & \vec v & \vec w \end{vmatrix} &= x_1 \begin{vmatrix} u_2 & v_2 & w_2 \\ u_3 & v_3 & w_3 \\ u_4 & v_4 & w_4 \end{vmatrix} - x_2 \begin{vmatrix} u_1 & v_1 & w_1 \\ u_3 & v_3 & w_3 \\ u_4 & v_4 & w_4 \end{vmatrix} + x_3 \begin{vmatrix} u_1 & v_1 & w_1 \\ u_2 & v_2 & w_2 \\ u_4 & v_4 & w_4 \end{vmatrix} - x_4 \begin{vmatrix} u_1 & v_1 & w_1 \\ u_2 & v_2 & w_2 \\ u_3 & v_3 & w_3 \end{vmatrix} \\ \\
                \vec z &= \begin{bmatrix}
                    \begin{vmatrix} u_2 & v_2 & w_2 \\ u_3 & v_3 & w_3 \\ u_4 & v_4 & w_4 \end{vmatrix} \\ - \begin{vmatrix} u_1 & v_1 & w_1 \\ u_3 & v_3 & w_3 \\ u_4 & v_4 & w_4 \end{vmatrix} \\ \begin{vmatrix} u_1 & v_1 & w_1 \\ u_2 & v_2 & w_2 \\ u_4 & v_4 & w_4 \end{vmatrix} \\ 
                    - \begin{vmatrix} u_1 & v_1 & w_1 \\ u_2 & v_2 & w_2 \\ u_3 & v_3 & w_3 \end{vmatrix}
                \end{bmatrix}
            \end{align*}
        \end{solution}
    \part Find the vector $\vec z$ (as in part (a)) when $\vec u = \vec e_1, \vec v = \vec e_2$, and $\vec w = \vec e_3$ are the ﬁrst three
    standard basis vectors in $\reals^4$.
        \begin{solution}
            When $u, v, w$ are such, only the last element of $\vec z$ is nonzero. So, $\vec z$ is $\begin{bmatrix*} 0 \\ 0 \\ 0 \\ -1 \end{bmatrix*}$.
        \end{solution}
    \part When is $\vec z = \vec 0$? (Your answer should be in terms of $\vec u, \vec v$, and $\vec w$.)
        \begin{solution}
            The determinant of a set of vectors is zero if and only if it is linearly dependent. When $\vec z$ is 0, this is an equivalent statement to the determinant $| \vec x \ \vec u \ \vec v \ \vec w |$ being 0 for all $\vec x$. This can only happen when $\vec u, \vec v, \vec w$ are linearly dependent.
            \par Alternatively, we can realize that $\vec z = 0$ means that $\vec u, \vec v, \vec w$ are linearly dependent since their corresponding vectors with one element removed are linearly dependent (e.g. $\begin{bmatrix}
                u_1 \\ u_2 \\ u_3
            \end{bmatrix}, \begin{bmatrix}
                v_1 \\ v_2 \\ v_3
            \end{bmatrix}, \begin{bmatrix}
                w_1 \\ w_2 \\ w_3
            \end{bmatrix}$ are linearly dependent since the last element of $\vec z$, the determinant of these three vectors, is 0).
        \end{solution}
    \part Prove that $\vec z$ is orthogonal to each of $\vec u, \vec v$ and $\vec w$, and ﬁnd det($[\vec z \ \vec u \ \vec v \ \vec w]$) in terms of $||\vec z||$.
        \begin{solution}
            We use $\vec u$ to represent any of $\vec u, \vec v, \vec w$ without loss of generality. The dot product $\vec u \cdot \vec z =$ det$([\vec u \ \vec u \ \vec v \ \vec w])$ by our definition of $\vec z$. Since this matrix has two columns of $\vec u$, it is linearly dependent. So its determinant is 0, and the dot product $\vec u \cdot \vec v = 0$. So $\vec z$ is orthogonal to $\vec u, \vec v, \vec w$.
            \par By our definition of $\vec z$, det($[\vec z \ \vec u \ \vec v \ \vec w]) = \vec z \cdot \vec z = ||\vec z||^2$.
        \end{solution}
\end{parts}

\question \begin{parts}
    \part Prove that for every $n \times n$ matrix $A$ and for every eigenvalue $\lambda$ of $A$, the real number $p(\lambda)$
    is an eigenvalue of the $n \times n$ matrix $p(A)$.
        \begin{solution}
            Let $\vec v$ be a eigenvector corresponding to $\lambda$ of $A$. Let $p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots$, where $a_i \in \reals$. Then 
                \begin{align*}
                    p(A) \vec v &= a_0 \vec v + a_1 A \vec v + a_2 A^2 \vec v + a_3 A^3 \vec v + \cdots \\
                    &= a_0 \vec v + a_1 \lambda \vec v + a_2 \lambda^2 \vec v + a_3 \lambda^3 \vec v + \cdots \tag{definition of eigenvector}\\
                    &= \left( a_0  + a_1 \lambda  + a_2 \lambda^2  + a_3 \lambda^3  + \cdots\right)\vec v \\
                    &= p(\lambda) \vec v
                \end{align*}
            So $p(\lambda)$ is an eigenvalue of $p(A)$.
        \end{solution}
    \part Let $p$ be a polynomial and let $n \in N$. Prove that if $S$ is an invertible $n \times n$ matrix, then for every $A \in \realsnxn$ we have $p(S^{-1}AS) = S^{-1}p(A)S$.
        \begin{solution}
            Let $p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots$, where $a_i \in \reals$. Then 
            \begin{align*}
                p(S^{-1}AS) &= a_0 I_n + a_1 (S^{-1}AS) + a_2 (S^{-1}AS)^2 + a_3 (S^{-1}AS)^3 + \cdots \\
                &= a_0 I_n + a_1 (S^{-1}AS) + a_2 S^{-1}A^2S + a_3 S^{-1}A^3S + \cdots \tag{since $(S^{-1}AS)^n = S^{-1} A^n S \  \forall n \in \naturals$}\\
                &= a_0 S^{-1}I_n S + a_1 (S^{-1}AS) + a_2 S^{-1}A^2S + a_3 S^{-1}A^3S + \cdots \\
                &= S^{-1} \left(a_0 I_n  + a_1 A + a_2 A^2 + a_3 A^3 + \cdots\right) S \\
                &= S^{-1} p(A) S \\
            \end{align*}
        \end{solution}
    \part Let $p$ be a polynomial and let $A$ be an $n \times n$ matrix. Prove that if $A$ is diagonalizable, then every eigenvalue of $p(A)$ is of the form $p(\lambda)$ for some eigenvalue $\lambda$ of $A$.
        \begin{solution}
            
        \end{solution}
\end{parts}

\question \begin{parts}
    \part Let $A \in \reals^{2 \times 2}$ be a $2 \times 2$ matrix such that $A^2 = I_2$. Prove that $A$ is diagonalizable. (Hint: try factoring $A^2 - I_2$, and consider the possible ranks of the factors.)
    \part Does the same result hold for larger matrices? That is, if $A \in \realsnxn$ is an $\nxn$ matrix for which $A^2 = I_n$, must $A$ be diagonalizable? Either prove this or give a counterexample.
\end{parts}
\end{questions}

\end{document}