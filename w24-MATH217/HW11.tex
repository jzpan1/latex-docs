\documentclass[12pt]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{mathrsfs}

\usepackage{bm} %bold symbols
\usepackage{hyperref} %add links

%%%%%%%%%%%%%%%%%%%%%%%%%
% 	Define vars here 	%
%%%%%%%%%%%%%%%%%%%%%%%%%

\def\hwName{Homework 11 Part B}
\author{Zhengyu James Pan} %use like \@author
\def\instructor{Dr. Paul Kessenich}
\def\email{jzpan@umich.edu}
\def\dueDate{SUNDAY, April 21}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\realsn}{\reals^{n}}
\newcommand{\nxn}{n\times n}
\newcommand{\realsnxn}{\reals^{\nxn}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\transpose}{^\top}

\makeatletter

\begin{document}
%Header
\pagestyle{head}
\firstpageheader{}{}{}
\header{MATH 217}{\hwName}{\thepage}

%Solution formatting
\printanswers
\unframedsolutions

%Top matter
{\parindent0in
\bf
\begin{center}
	MATH 217 W24 - LINEAR ALGEBRA, Section 001 ({\instructor}) \\
	{\hwName} due {\dueDate} at 11:59pm \\
	\@author\ (\href{mailto:\email}{\email})
\end{center}
}

%1
\begin{questions}
\question 
	\begin{parts}
		\part Let $E_0$ denote the 0-eigenspace of $T$. Explicitly describe $E_0$ (as a set).
            \begin{solution}
                \[ E_0 = \left\{ (x_1, 0, x_2, 0, x_3, 0, ...) \mid x_i \in \reals \right\} \]
            \end{solution}
        \part Prove that every real number $\lambda$ is an eigenvalue of $T$. (Hint: explicitly construct an eigenvector $(x_1, x_2, x_3, . . . ) \in V$ . First consider $x_i$ when $i$ is a power of 2.)
            \begin{solution}
                Let $\lambda \in \reals$. Then let \[ s = (1,\  \lambda, \lambda,\  \lambda^2, \lambda^2, \lambda^2, \lambda^2,\  \lambda^3, \lambda^3, \lambda^3, \lambda^3, \lambda^3, \lambda^3, \lambda^3, \lambda^3,\  ...) \]
                be an infinite sequence such that each consecutive power $\lambda^n$ is repeated $n$ times in the sequence, starting from $n=0$. Then
                \begin{align*}
                    T(s) &= (\lambda,\ \lambda^2, \lambda^2,\ \lambda^3, \lambda^3, \lambda^3, \lambda^3, \ \lambda^4, \lambda^4, \lambda^4, \lambda^4, \lambda^4, \lambda^4, \lambda^4, \lambda^4, ...  )\\
                    &= \lambda(s).
                \end{align*}
                So any real number is an eigenvalue of $T$.
            \end{solution}
	\end{parts}

    \clearpage
%2
\question 
    \begin{parts}
        \part Let $\mathscr{D}$ be a diagonal $\nxn$ matrix with distinct entries along the diagonal, and let $\mathscr D$ be the subset of $\realsnxn$ consisting of all diagonal matrices. Prove $\mathscr C (D) = \mathscr D$.
            \begin{solution}
                Let the diagonal entries of $D$ be $d_1, ..., d_n$. Let $A \in \mathscr D$, with diagonal entries $a_1, ... a_n$. Then the product \[AD = \begin{bmatrix}
                    a_1 d_1 & 0 & \dots & 0\\
                    0 & a_2 d_2 & 0 & \vdots \\
                    \vdots & 0 & \ddots & 0  \\
                    0 & \dots & 0 & a_n d_n
                \end{bmatrix} = \begin{bmatrix}
                    d_1 a_1 & 0 & \dots & 0\\
                    0 & d_2 a_2 & 0 & \vdots \\
                    \vdots & 0 & \ddots & 0  \\
                    0 & \dots & 0 & d_n a_n
                \end{bmatrix} = DA. \]
                So $\mathscr D \subset \mathscr C (D)$. \\
                \par Let $B \in \mathscr C(D)$ with columns $\vec b_1, ..., \vec b_n$, rows $\vec c_1, ..., \vec c_n$, and element of $i$th row and $j$th column $b_{ij}$. Then
                    \begin{align*}
                        BD &= \begin{bmatrix}
                            \mid &  & \mid \\
                            B (d_1 \vec e_1) & \cdots & B (d_n \vec e_n) \\
                            \mid &  & \mid
                        \end{bmatrix} \\
                        &= \begin{bmatrix}
                            \mid &  & \mid \\
                            d_1 \vec b_1  & \cdots & d_n \vec b_n \\
                            \mid &  & \mid
                        \end{bmatrix} \\\\
                        DB &=\left((DB)\transpose\right)\transpose \\
                        &= \left(B\transpose D\right)\transpose \\
                        &= \begin{bmatrix}
                            \mid &  & \mid \\
                            d_1 \vec c_1\transpose  & \cdots & d_n \vec c_n\transpose \\
                            \mid &  & \mid
                        \end{bmatrix}\transpose \\
                        &= \begin{bmatrix}
                            \rule[3pt]{0.45cm}{0.5pt} & d_1 \vec c_1 & \rule[3pt]{0.45cm}{0.5pt} \\
                             & \vdots & \\
                            \rule[3pt]{0.45cm}{0.5pt} & d_n \vec c_n & \rule[3pt]{0.45cm}{0.5pt}
                        \end{bmatrix}
                    \end{align*}
                where $\vec e_i$ is the $i$th standard basis vector. Since $B \in \mathscr C(D), BD = DB$. Considering arbitrary $b_{ij}$, this means that $d_i b_{ij} = d_j b_{ij}$. 
                \par When $i=j$, then $d_i = d_j$, so $b_{ij}$, a diagonal element of $B$, can be anything.
                \par When $i \neq j$, then $d_i \neq d_j$ since $D$ has distinct diagonal elements. But $d_i b_{ij} = d_j b_{ij}$. So $b_ij = 0$. Note that in this case, $b_ij$ is any non-diagonal element.
                Since only diagonal elements of $B$ can be nonzero, $B$ is diagonal, and $B \in \mathscr D$. So $\mathscr C(D) \subset \mathscr D$.
                \par Thus $\mathscr D = \mathscr C(D).$ 
            \end{solution}
        \part Prove that if $A$ and $B$ are simultaneously diagonalizable $\nxn$ matrices, then $B \in \mathscr C (A)$. 
            \begin{solution}
                We know that $\mathscr D \subset \mathscr C(D)$ for any diagonal matrix $D$ by part (a). (The $\mathscr D \subset \mathscr C (D)$ direction did not depend on the fact that $D$ had distinct diagonal elements.) So then
                \begin{align*}
                    S^{-1} B S S^{-1} A S &= S^{-1} A S S^{-1} B S \\
                    S^{-1} B A S &= S^{-1} A B S \\
                    B A &= A B
                \end{align*}
                Thus $B \in \mathscr C(A)$.
            \end{solution}
        \part Prove that if $A$ and $B$ are $\nxn$ matrices such that $A$ has $n$ distinct eigenvalues and $B \in \mathscr C (A)$, then $A$ and $B$ are simultaneously diagonalizable.
            \begin{solution}
                Let $S$ be an eigenbasis of $A$. Then $S^{-1} A S$, the diagonalized matrix of $A$, has distinct diagonal elements. We know that 
                \begin{align*}
                    B A &= A B \\
                    S^{-1} B A S &= S^{-1} A B S \\
                    S^{-1} B S S^{-1} A S &= S^{-1} A S S^{-1} B S
                \end{align*}
                So $S^{-1} B S \in \mathscr C(S^{-1} A S)$. Then by part (a), since $S^{-1} A S$ is diagonal with distinct entries, $S^{-1} B S$ is diagonal. So $S$ diagonalizes both $B$ and $A$. So $A$ and $B$ are simultaneously diagonalizable.
            \end{solution}
    \end{parts}

    \clearpage

\question \begin{parts}
    \part Suppose that $A$ has eigenvalue 0 but is not diagonalizable. Prove that im($A) = E_0$, and conclude from this that $A^2 = 0$.
        \begin{solution}
            The givens imply that det($A - 0I_1) = 0$. So det($A) = 0$. But since it is not diagonalizable, there must be one eigenvalue $\lambda$ of $A$ which does not satisfy almu($\lambda) = $ gemu($\lambda$). Since both almu and gemu are always at least 1, almu $\geq$ gemu, and the sum of almus = $n$; we know almu(0) = 2, and gemu(0) = 1. From this, we infer that the nullity is 1, and the rank is also 1.  The almu also implies that the characteristic equation of $A$ is $x^2 = 0$. \\
            Let $A = \begin{bmatrix*} a & b \\ c & d \end{bmatrix*}.$ Since \begin{align*}
                \text{det}(A - 0I_1) &= (a - x)(d - x) - bc \\
                &= ad - bc - (a + d)x + x^2
            \end{align*} 
            and the characteristic polynomial has no $x^1$ terms, we know that $a + d = 0$, or equivalently that $a = -d$. Additionally, $ad = bc$.
            \par We know that $A$ does not have linearly independent columns, so the columns must be scalar multiples of each other. In fact, since $a = -d$, the ratio of the second column to the first is $\dfrac{d}{c} = \dfrac{-a}{c}$. So then the rref form of $A$ is 
            \[ \begin{bmatrix}
                1 & \dfrac{-a}{c} \\ 0 & 0
            \end{bmatrix} \]
            which has nullspace spanned by $\begin{bmatrix} a \\ b \end{bmatrix}$. But this is also the image of $A$, since both columns of $A$ are scalar multiples of $\begin{bmatrix} a \\ b \end{bmatrix}$. So the image and nullity of $A$ are the same. Thus 
                \[ A^2\vec x = A(A\vec x) = 0 \]
            for any $\vec x \in \reals^2$. Since $A \vec x$ is in ker($A$). So $A^2 = 0$.
        \end{solution}
    \part Let $\lambda \in \reals$ and suppose that $A$ has eigenvalue $\lambda$ but is not diagonalizable. Prove that we have $(A - \lambda I_2)^2$ = 0, and deduce from this that $A \vec v - \lambda \vec v \in E_\lambda$ for every $\vec v \in \reals^2$.
    [Hint: apply part (a) to the matrix $A - \lambda I_2$].
        \begin{solution}
            Since both almu and gemu are always at least 1, almu $\geq$ gemu, and the sum of almus = $n$; we know almu($\lambda$) = 2, and gemu($\lambda) = 1$. So the matrix $A - \lambda I_2$ has an eigenvalue of 0, and nullity 1. So $A - \lambda I_2$ has an eigenvalue of 0 and is not diagonalizable. 
            \par Then by (a), $(A - \lambda I_2)^2 = 0$, and the image of $A - \lambda I_2$ is equal to its 0-eigenspace. The 0-eigenspace of $A - \lambda I_2$ is in turn equal to its own kernel. We know ker($A - \lambda I_2$) is also the $\lambda$-eigenspace of $A$. So for all $\vec v \in \reals^2$, $(A - \lambda I_n) \vec v = A \vec v - \lambda \vec v \in E_\lambda$.
        \end{solution}
    \part Prove that if $A$ has eigenvalue $\lambda$ but is not diagonalizable, then $A$ is similar to $\begin{bmatrix}
        \lambda & 1 \\ 0 & \lambda
    \end{bmatrix}$.
        \begin{solution}
            By (b), we know that $A \vec v - \lambda \vec v \in E_\lambda$ for any vector $\vec v \in \reals^2$. Let $\mathcal B = \{\vec b_1, \vec b_2\},$ where $\vec b_2 \in E_\lambda^\perp$ and $\vec b_1 = A \vec b_2 - \lambda \vec b_2$, which is in $E_\lambda$ by (b). We know that these are linearly independent since they are orthogonal.
            \par Note that this definition of $\mathcal B$ ensures that $A \vec b_1 = \lambda \vec b_1$ and $A \vec b_2 = \vec b_1 + \lambda \vec b_2$.
            \par Then 
                \begin{align*}
                    [A]_\mathcal B &= \begin{bmatrix*} [A\vec b_1]_\mathcal B & [A\vec b_2]_\mathcal B
                    \end{bmatrix*} \\
                    &= \begin{bmatrix}
                        \lambda & 1 \\ 0 & \lambda
                    \end{bmatrix}
                \end{align*}
            So $A$ is similar to $\begin{bmatrix}
                \lambda & 1 \\ 0 & \lambda
            \end{bmatrix}$ by the Change-of-basis of Theorem.
        \end{solution}
    \part Prove that if $A$ does not have any real eigenvalues, then $A$ is similar to a matrix of the form $\lambda Q$ where $Q$ is an orthogonal matrix and $\lambda > 0$.
        \begin{solution}
            We know $A$ has a pair of complex eigenvalues of the form $a \pm bi$. By Worksheet 26 problem 9, $A$ is then similar to the matrix $B = \begin{bmatrix}
                a & b \\ -b & a
            \end{bmatrix}$. Since the columns are already orthogonal, we simply need to normalize them to make the matrix orthogonal, which can be done by dividing by $\lambda = \sqrt{a^2 + b^2}$, which is a positive number. So $Q = \frac{1}{\lambda} B$, and the statement is true.
        \end{solution}
\end{parts}
\clearpage
\question \begin{parts}
    \part Find a matrix $A \in \reals ^{2 \times 2}$ such that $A \begin{bmatrix*} x_n \\ x_{n+1} \end{bmatrix*} = \begin{bmatrix*} x_{n+1} \\ x_{n+2} \end{bmatrix*}$ for every integer $n \geq 0$.
        \begin{solution}
            \[\boxed{A = \begin{bmatrix}
                0 & 1 \\ -13 & 4
            \end{bmatrix} }\]
            \[ A \begin{bmatrix*} x_n \\ x_{n+1} \end{bmatrix*} = \begin{bmatrix*} 0x_n + x_{n+1} \\ -13x_n + 4x_{n+1} \end{bmatrix*} = \begin{bmatrix*} x_{n+1} \\ x_{n+2} \end{bmatrix*} \]
        \end{solution}
    \part Use part (a) to prove by induction that your matrix $A$ satisfies $A^n \begin{bmatrix} 0 \\ 2 \end{bmatrix} = \begin{bmatrix} x_n \\ x_{n+1} \end{bmatrix}$ for every $n \geq 0$.
        \begin{solution}
            Base case: $A^0 \begin{bmatrix} 0 \\ 2 \end{bmatrix} = I_2 \begin{bmatrix} 0 \\ 2 \end{bmatrix}$
            \par \textbf{Inductive step:} Assume that for some integer $n \geq 0$, \[A^n \begin{bmatrix} 0 \\ 2 \end{bmatrix} = \begin{bmatrix} x_n \\ x_{n+1} \end{bmatrix}.\]
            Then by part (a),
            \begin{align*}
                A A^n \begin{bmatrix} 0 \\ 2 \end{bmatrix} &= A^{n+1} \begin{bmatrix} 0 \\ 2 \end{bmatrix} \\
                &= A \begin{bmatrix} x_n \\ x_{n+1} \end{bmatrix} \\
                &= \begin{bmatrix} x_{n+1} \\ x_{n+2} \end{bmatrix}
            \end{align*}
            So if $n$ satisfies the statement, then $n+1$ satisfies the statement.
            Since $n = 0$ satisfies the statement, the statement is true for all $n \geq 0$.
        \end{solution}
    \part Find all (real or complex) eigenvalues and corresponding eigenvectors for $A$.
        \begin{solution}
            \begin{align*}
                \begin{vmatrix}
                    0 - \lambda & 1 \\ -13 & 4 - \lambda 
                \end{vmatrix} &= (- \lambda) ( 4 - \lambda) + 13 \\
                &= \lambda^2 - 4\lambda + 13 \\\\
                \lambda = \frac{4 \pm \sqrt{16 - 52}}{2} &= 2 \pm 3i
                \\\\
                2+3i:\\
                \begin{bmatrix*}
                    - 2 - 3i & 1 \\ -13 & 2 - 3i
                \end{bmatrix*} & \rightarrow \begin{bmatrix*}
                    - 2 - 3i & 1 \\ 0 & 0
                \end{bmatrix*} \\
                E_{2 + 3i} &= \text{span}\left(\begin{bmatrix*}1 \\ 2 + 3i
                \end{bmatrix*}\right)
                \\\\
                2-3i:\\
                \begin{bmatrix*}
                    - 2 + 3i & 1 \\ -13 & 2 + 3i
                \end{bmatrix*} & \rightarrow \begin{bmatrix*}
                    - 2 + 3i & 1 \\ 0 & 0
                \end{bmatrix*} \\
                E_{2 + 3i} &= \text{span}\left(\begin{bmatrix*}1 \\ 2 - 3i
                \end{bmatrix*}\right)
            \end{align*}
        \end{solution}
    \part Find an invertible (real or complex) matrix $P$ such that $A = P DP^{-1}$ where $D$ is a diagonal matrix.
        \begin{solution}
            \[ P = \begin{bmatrix}
                1 & 1 \\ 2 + 3i & 2 - 3i
            \end{bmatrix} \]
        \end{solution}
    \part First give an explicit formula for $D_n$, and then use this to give an explicit formula for $A_n$.
        \begin{solution}
            $D = \begin{bmatrix}
                2 + 3i & 0 \\ 0  & 2 - 3i
            \end{bmatrix}$, so 
            \[D^n = \begin{bmatrix}
                (2 + 3i)^n & 0 \\ 0  & (2 - 3i)^n
            \end{bmatrix} \]
            So then 
            \[ A^n = P^{-1} D^n P = \begin{bmatrix}
                1 & 1 \\ 2 + 3i & 2 - 3i
            \end{bmatrix}^{-1} \begin{bmatrix}
                (2 + 3i)^n & 0 \\ 0  & (2 - 3i)^n
            \end{bmatrix} \begin{bmatrix}
                1 & 1 \\ 2 + 3i & 2 - 3i
            \end{bmatrix}. \]
        \end{solution}
    \part Using parts (b) and (e), give an explicit formula for $x_n$, the $n$th term in the sequence. (Your formula may involve complex numbers, and need not be fully simplified.)
        \begin{solution}
            \begin{align*}
                \begin{bmatrix}
                    x_n \\ x_{n+1}
                \end{bmatrix} &= A^n\begin{bmatrix}
                    0 \\ 2
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    1 & 1 \\ 2 + 3i & 2 - 3i
                \end{bmatrix}^{-1} \begin{bmatrix}
                    (2 + 3i)^n & 0 \\ 0  & (2 - 3i)^n
                \end{bmatrix} \begin{bmatrix}
                    1 & 1 \\ 2 + 3i & 2 - 3i
                \end{bmatrix} \begin{bmatrix}
                    0 \\ 2
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    1 & 1 \\ 2 + 3i & 2 - 3i
                \end{bmatrix}^{-1} \begin{bmatrix}
                    (2 + 3i)^n & 0 \\ 0  & (2 - 3i)^n
                \end{bmatrix} \begin{bmatrix}
                    2 \\ 4 - 3i
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    1 & 1 \\ 2 + 3i & 2 - 3i
                \end{bmatrix}^{-1} \begin{bmatrix}
                    2(2 + 3i)^n \\ (4 - 3i) (2 - 3i)^n
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    \frac{3 + 2i}{6} & \frac{-i}{6} \\ \frac{3 - 2i}{6} & \frac i6
                \end{bmatrix} \begin{bmatrix}
                    2(2 + 3i)^n \\ (4 - 3i) (2 - 3i)^n
                \end{bmatrix} \\\\
                x_n &= \frac{3 + 2i }{6} \cdot 2(2 + 3i )^n -\frac{i}{6} (4 - 3i) (2 - 3i)^n
            \end{align*}
        \end{solution}
\end{parts}


\clearpage
%5
\question \begin{parts}
    \part Find $[T]_\mathcal A$.
        \begin{solution}
            \begin{align*}
                [T]_\mathcal A &= \begin{bmatrix}
                    [T(e^{3x})]_\mathcal A & [T(\cos 2x)]_\mathcal A & [T(\sin 2x)]_\mathcal A
                \end{bmatrix} \\
                [T(e^{3x})]_\mathcal A &= [3e^{3x}]_\mathcal A = \begin{bmatrix}
                    3 \\ 0 \\ 0
                \end{bmatrix} \\
                [T(\cos 2x)]_\mathcal A &= [-2\sin 2x]_\mathcal A = \begin{bmatrix}
                    0 \\ 0 \\ -2
                \end{bmatrix}\\
                [T(\sin 2x)]_\mathcal A &= [2\cos 2x]_\mathcal A = \begin{bmatrix}
                    0 \\ 2 \\ 0
                \end{bmatrix}\\\\
                [T]_\mathcal A &= \begin{bmatrix}
                    3 & 0 & 0 \\ 0 & 0 & 2 \\ 0 & -2 & 0
                \end{bmatrix}
            \end{align*}
        \end{solution}
    \part Find all (real or complex) eigenvalues of the matrix $[T]_\mathcal A$.
        \begin{solution}
            We find the characteristic polynomial.
            \begin{align*}
                \begin{vmatrix}
                    3 - \lambda & 0 & 0 \\ 0 & 0 - \lambda & 2 \\ 0 & -2 & 0-\lambda
                \end{vmatrix} &= (3 - \lambda)(- \lambda)^2 + 4(3 - \lambda) \\\\
                (3 - \lambda)(\lambda^2 + 4)  &= 0 \\
                \boxed{\lambda = 3, \pm 2i}
            \end{align*}
        \end{solution}
    \part Viewing the matrix $[T]_\mathcal A$ as a linear transformation of the complex vector space $\mathbb C^3$, find a complex eigenvector for $[T]_\mathcal A$ for each of the eigenvalues you found in (b).
        \begin{solution}
            \begin{align*}
                [T]_\mathcal A\begin{bmatrix}
                    1 \\ 0 \\ 0
                \end{bmatrix} &= 3 \begin{bmatrix}
                    1 \\ 0 \\ 0
                \end{bmatrix} \\
                [T]_\mathcal A\begin{bmatrix}
                    0 \\ -i \\ 1
                \end{bmatrix} &= (2i) \begin{bmatrix}
                    0 \\ -i \\ 1
                \end{bmatrix} \\
                [T]_\mathcal A\begin{bmatrix}
                    0 \\ i \\ 1
                \end{bmatrix} &= (-2i) \begin{bmatrix}
                    0 \\ i \\ 1
                \end{bmatrix} \\
            \end{align*}
        \end{solution}
    \part Interpret the eigenvectors you found in (c) as a set of three complex-valued functions
     \[ \mathcal B = (f_1(x), f_2(x), f_3(x)) \]
    with the property that any complex linear combination of the vectors in $A$ (that is, a linear combination with coefficients in $\complex$) can be written as a complex linear combination of the vectors in $\mathcal B$, and vice versa. 
        \begin{solution}
            \[ \boxed{\mathcal B = \left( e^{3x}, \sin(2x) - i \cos(2x), i \cos (2x) + \sin(2x)\right)} \]
        \end{solution}
\end{parts}

\clearpage

\question \begin{parts}
    \part Show that if $I$ has a real eigenvalue $\lambda$ than there exists an axis around which the solid object can rotate without wobbling. 
        \begin{solution}
            Assume $I$ has a real eigenvalue $\lambda$. Then let $\vec \omega$ be in the $\lambda$-eigenspace of $I$. Then $L = I \vec \omega = \lambda \vec \omega$. So $L$ and $\vec \omega$ point in the same direction, since they are scalar multiples. Thus along the axis of rotation of the $\lambda$-eigenspace of $I$, the solid object will not wobble.
        \end{solution}
    \part Show that $I$ always has at least one real eigenvalue $\lambda$ (and hence by (a) there always exists an axis around which a solid object can rotate without wobbling).
        \begin{solution}
            By the intermediate value theorem, an odd polynomial must always pass through the origin. Thus, it always has one real root. We know that the characteristic polynomial of $I$ has degree 3 by Theorem 7.2.5. So $I$ always has at least one real root, and thus one non-wobbling axis of rotation. 
        \end{solution}
    \part Show that if gemu($\lambda$) = 3 then the solid object can rotate around any axis without wobbling.
        \begin{solution}
            If gemu($\lambda$) = 3, then $I - \lambda I_3$ has nullity 3, so $I - \lambda I_3 = 0$. Thus $I = \lambda I_3$. So for any $\vec \omega \in \reals^3$, $L = \lambda I_3 \vec \omega = \lambda \omega$. So for any axis of angular velocity, the angular momentum will be in the same direction, and no wobbling will occur.
        \end{solution}
    \part Show that if $I$ has three distinct real eigenvalues then there exist three axes around which the solid object can rotate without wobbling.
        \begin{solution}
            Let $I$ have distinct eigenvalues $\lambda_1, \lambda_2, \lambda_3$. Then the eigenspaces corresponding to these eigenvalues are all axes about which the solid object can rotate without wobbling. WLOG (between $\lambda_1, \lambda_2, \lambda_3$) let  $\vec \omega_1 \in E_{\lambda_1}$. Then $L = I \vec \omega_1 = \lambda \vec \omega_1$. So the object will not wobble about the axis of the $\lambda_1$-eigenspace. 
            \par Similarly for the $\lambda_2$- and $\lambda_3$-eigenspaces. So there exist 3 axes about which the solid object does not wobble.
        \end{solution}
    \part Prove that for any solid object, there exist three perpendicular axes of rotation around which the object will not wobble.
        \begin{solution}
            From the formula for the $(i, j)$-component, the $(i,j)$ component is the same as the $(j, i)$ component (by commutativity of multiplication). So $I$ is symmetric. Then by the Spectral Theorem, there exists an orthogonal $S$ such that $S^{-1}IS$ is diagonal. Then $S$ is an orthogonal eigenbasis of $I$. 
            \par By part (d) we know that the eigenspaces of $I$ correspond to the axes around which the object will not wobble. From the characteristics of $S$, we know that the eigenspaces are orthogonal. So the axes of rotation around which the object will not wobble are orthogonal.
        \end{solution}
\end{parts}

\end{questions}

\end{document}